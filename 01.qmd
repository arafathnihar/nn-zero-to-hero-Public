---
title: "The spelled-out intro to neural networks and backpropagation: building micrograd"
format:
    html:
        code-fold: false
jupyter: python3
---

```{python}
import math
import numpy as np
import matplotlib.pyplot as plt
```

## derivative of a simple function with one input
```{python}
def f(x):
    return 3*x**2 - 4*x + 5
f(3.0)
```
```{python}
f(2.0)
```    
```{python}
xs = np.arange(-5, 5, 0.25)
xs
```
```{python}
ys = f(xs)
ys
```

```{python}
plt.plot(xs, ys)
```
The derivative is a fundamental tool of calculus that quantifies the sensitivity of change of a function's output with respect to its input. The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point.

```{python}
h = 0.001
x = 3.0
f(x)
```
```{python}
f(x+h)
```
![Derivative](derivDef.webp)
```{python}
(f(x+h) - f(x))/h
```
```{python}
h = 0.000001
x = 3.0
(f(x+h) - f(x))/h
```
```{python}
h = 0.00000001
x = 3.0
(f(x+h) - f(x))/h
```
```{python}
h = 0.0000000000000001
x = 3.0
(f(x+h) - f(x))/h
```

```{python}
h = 0.00000001
x = -3.0
(f(x+h) - f(x))/h
```

```{python}
h = 0.00000001
x = 2/3
(f(x+h) - f(x))/h
```
```{python}
h = 0.000001
x = 2/3
(f(x+h) - f(x))/h
```
## derivative of a function with multiple inputs
```{python}
a = 2.0
b = -3.0
c = 10.0
d = a*b + c
d
```
```{python}
a = 2.0
b = -3.0
c = 10.0
h = 0.0001
d1 = a*b + c
a += h
d2 = a*b + c
print('d1:', d1)
print('d2:', d2)
print('Slope (Df(x)/Da) = b : ', (d2 - d1)/h)

```

```{python}
a = 2.0
b = -3.0
c = 10.0
h = 0.0001
d1 = a*b + c
b += h
d2 = a*b + c
print('d1:', d1)
print('d2:', d2)
print('Slope (Df(x)/Db) = a : ', (d2 - d1)/h)
```

```{python}
a = 2.0
b = -3.0
c = 10.0
h = 0.0001
d1 = a*b + c
c += h
d2 = a*b + c
print('d1:', d1)
print('d2:', d2)
print('Function incrases by h; So slope is 1 : ', (d2 - d1)/h)
```

## starting the core Value object of micrograd and its visualization


neural networks will be pretty massive expressions mathematical expressions so we need some data structures that maintain these expressions and that's what we're going to start to build out

```{python}
class Value:
    def __init__(self, data) -> None:
        self.data = data

    def __repr__(self) -> str:
        return f"Value(data={self.data})"
    
    def __add__(self, other) -> Value:
        return Value(self.data + other.data)

    def __mul__(self. other) -> Value:
        return Value(self.data * other.data)

```